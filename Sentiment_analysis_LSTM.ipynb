{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4d7b444-56e1-4d44-8d8d-baadb69eccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 299ms/step - accuracy: 0.7372 - loss: 0.5093 - val_accuracy: 0.8520 - val_loss: 0.3568\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 290ms/step - accuracy: 0.8783 - loss: 0.3002 - val_accuracy: 0.8516 - val_loss: 0.3430\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 299ms/step - accuracy: 0.9194 - loss: 0.2085 - val_accuracy: 0.8496 - val_loss: 0.3676\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 295ms/step - accuracy: 0.9432 - loss: 0.1593 - val_accuracy: 0.8734 - val_loss: 0.3809\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 301ms/step - accuracy: 0.9575 - loss: 0.1191 - val_accuracy: 0.8628 - val_loss: 0.3776\n",
      "Test accuracy: 0.858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# 1ï¸âƒ£ Load dataset (only top 10,000 most frequent words)\n",
    "max_words = 10_000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# 2ï¸âƒ£ Pad sequences so all reviews are same length\n",
    "max_len = 200\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test  = sequence.pad_sequences(x_test,  maxlen=max_len)\n",
    "\n",
    "# 3ï¸âƒ£ Build RNN model\n",
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "    layers.LSTM(128, activation=\"tanh\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# 4ï¸âƒ£ Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 5ï¸âƒ£ Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ Evaluate on test data\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae42250e-4b80-4567-95f9-11d9bf26112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive ğŸ˜€ 0.9924509525299072\n",
      "Negative ğŸ˜  0.07806771993637085\n",
      "Negative ğŸ˜  0.002139448653906584\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§° Step 1: Import necessary libraries\n",
    "import re, numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# âš™ï¸ Step 2: Define important constants (same as used during training)\n",
    "max_words = 10_000   # use top 10,000 common words\n",
    "max_len   = 200      # every review = 200 words\n",
    "index_from = 3       # IMDB reserves 0,1,2 for special tokens\n",
    "\n",
    "# ğŸ—ºï¸ Step 3: Load IMDB's word index and adjust it (+3 shift)\n",
    "raw_word_index = imdb.get_word_index()\n",
    "word_index = {w:(i+index_from) for w,i in raw_word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# âœ‚ï¸ Step 4: Make a simple word splitter (keeps only letters and apostrophes)\n",
    "_tok = re.compile(r\"[a-zA-Z']+\")\n",
    "\n",
    "# ğŸ”¢ Step 5: Convert a text review into a sequence of numbers\n",
    "def encode_review(text, max_words=max_words, max_len=max_len):\n",
    "    tokens = _tok.findall(text.lower())   # make lowercase & split into words\n",
    "    seq = [1]  # start with <START> token\n",
    "    for t in tokens:\n",
    "        idx = word_index.get(t, 2)        # get word id or <UNK>\n",
    "        if idx >= max_words:              # too rare? mark as unknown\n",
    "            idx = 2\n",
    "        seq.append(idx)\n",
    "    return sequence.pad_sequences([seq], maxlen=max_len)  # pad to 200 length\n",
    "\n",
    "# ğŸ§  Step 6: Make a prediction (Positive or Negative)\n",
    "def predict_sentiment(review, threshold=0.5, mdl=None):\n",
    "    x = encode_review(review)                       # prepare text\n",
    "    p = float(mdl.predict(x, verbose=0)[0][0])      # get probability\n",
    "    label = \"Positive ğŸ˜€\" if p >= threshold else \"Negative ğŸ˜ \"\n",
    "    return label, p\n",
    "\n",
    "# ğŸ¬ Step 7: Try it with your trained model\n",
    "label, prob = predict_sentiment(\"This movie was absolutely amazing! The story was emotional, the acting was brilliant, and the visuals were stunning\", mdl=model); print(label, prob)\n",
    "label, prob = predict_sentiment(\"The film was painfully slow, the dialogue was terrible.\", mdl=model); print(label, prob)\n",
    "label, prob = predict_sentiment(\"It was boring, slow, and poorly written.\", mdl=model); print(label, prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65200e9a-e6e1-4080-a5ab-4a063090cea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
